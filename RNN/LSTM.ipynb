{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python libraries\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "\n",
    "#other ML libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "#ploting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random seeds for reproducibility\n",
    "random_seed = 420\n",
    "np.random.seed(random_seed)\n",
    "_ = torch.manual_seed(random_seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Disruptive & Netatmo Sensor Data\n",
    "\n",
    "# assign variables\n",
    "directory = './storedData/'\n",
    "sensor_data = pd.DataFrame()\n",
    "sensor_data['timestamp'] = None #in order to merge based on it \n",
    "\n",
    "measurement_types = [\"Door\", \"Water\", \"Humidity_temperature\", \"Humidity_humidity\", \n",
    "                     \"Humidifier_humidity\", \"Humidifier_temperature\",\"Temperature\",\n",
    "                     \"Hum_temperature\", \"Hum_humidity\"]\n",
    "discrete_measurement_types = [\"Door\", \"Water\"]\n",
    "\n",
    "\n",
    "# iterate over files in that directory and concat into 1 big dataframe based on timestamps\n",
    "for filename in os.listdir(directory): \n",
    "    file = os.path.join(directory, filename)\n",
    "    if os.path.isfile(file):    \n",
    "        df = pd.read_csv(file)\n",
    "        df.drop(columns=df.columns[0], inplace=True) # dropping unnamed coloumn for indexing in .cvs file\n",
    "\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'],  utc=True)#.dt.tz_convert('Europe/Rome') #set timezone correctly if not correct in local time\n",
    "        df.set_index(\"timestamp\", inplace=True)\n",
    "        for measurement_name in df.columns:\n",
    "            measurement_frame = df[measurement_name].to_frame()\n",
    "\n",
    "            measurement_name_list = re.findall(\"([A-Z0-9][^A-Z]*)\", measurement_name) # Splitting string on capital letters\n",
    "            print(f'filename: {filename}, measurement_name_list: {measurement_name_list}')\n",
    "            measurement_type = measurement_name_list[-1]\n",
    "            if measurement_type not in measurement_types: #Temperature measurements does not have temperature in their name\n",
    "                measurement_type = \"Temperature\"\n",
    "                measurement_name_list.append(measurement_type)\n",
    "            #Renaming coloumn for consistent naming\n",
    "            coloumn_name = f\"{''.join(measurement_name_list[:-1])}_{measurement_type.lower()}\"\n",
    "            measurement_frame.rename(columns={measurement_frame.columns[0]: coloumn_name}, inplace=True)\n",
    "            # print(f\"key: {measurement_frame.columns[0]} Value: {coloumn_name}\")\n",
    "            if measurement_type in discrete_measurement_types:\n",
    "                #For Water Detection:\n",
    "                #NOT_PRESENT == No water detected = 0\n",
    "                #PRESENT     == water detected    = 1\n",
    "\n",
    "                #For Door Sensors: - this is because sensor changes state based on if it detects objects in front of it\n",
    "                #NOT_PRESENT == DOOR IS OPEN     = 0\n",
    "                #PRESENT     == DOOR IS CLOSED   = 1\n",
    "                \n",
    "                measurement_frame.replace(['NOT_PRESENT', 'PRESENT'],[0, 1], inplace=True)\n",
    "                measurement_frame = measurement_frame.resample('15T').ffill()\n",
    "            else: #Contious measurement\n",
    "                measurement_frame  = measurement_frame.resample('15T').mean(numeric_only=True).interpolate() #Resample to managable timestamps by taking mean of 5 minute slots and linear interpolating NaNs, i.e. 5, 10, 15, 20 minutes etc..\n",
    "\n",
    "    \n",
    "            sensor_data = pd.merge(sensor_data, measurement_frame, on='timestamp', how=\"outer\")  \n",
    "\n",
    "sensor_data.set_index(\"timestamp\", inplace=True)\n",
    "sensor_data.sort_index(axis=0, inplace=True) #Sorting by timestamp\n",
    "sensor_data.sort_index(axis=1, inplace=True) #Sorting columns alpabetically\n",
    "\n",
    "#Postprocesing the data in order to get rid of the NaN Values\n",
    "#ffill and fillna(0/1) to populate NaN values of the proximity with ffill and then set the values before first valid value to DOOR_CLOSED/NO_WATER\n",
    "sensor_data.loc[:, sensor_data.columns.str.endswith('door')] = sensor_data.loc[:, sensor_data.columns.str.endswith('door')].ffill().fillna(1)\n",
    "sensor_data.loc[:, sensor_data.columns.str.endswith('water')] = sensor_data.loc[:, sensor_data.columns.str.endswith('water')].ffill().fillna(0)\n",
    "\n",
    "\n",
    "# # #linearly interpolate temperature and humidity values:\n",
    "sensor_data.loc[:, sensor_data.columns.str.endswith('temperature')]  = sensor_data.loc[:, sensor_data.columns.str.endswith('temperature')].interpolate().bfill().ffill()\n",
    "sensor_data.loc[:, sensor_data.columns.str.endswith('humidity')]  = sensor_data.loc[:, sensor_data.columns.str.endswith('humidity')].interpolate().bfill().ffill()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLORATORY DATA ANALYSIS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the data\n",
    "sensor_data.plot(figsize=(16, 16))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE ENGINEERING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add hand-designed time-of-day and day-of-year features\n",
    "min_in_hour = 60\n",
    "min_in_day = 24*min_in_hour\n",
    "sensor_data['sin_daily'] = np.sin(2*np.pi*(sensor_data.index.minute + sensor_data.index.hour*min_in_hour)/min_in_day).values\n",
    "sensor_data['cos_daily'] = np.cos(2*np.pi*(sensor_data.index.minute + sensor_data.index.hour*min_in_hour)/min_in_day).values\n",
    "\n",
    "days_in_year = 365.2425\n",
    "sensor_data['sin_yearly'] = np.sin(2*np.pi*(sensor_data.index.day_of_year)/days_in_year).values\n",
    "sensor_data['cos_yearly'] = np.cos(2*np.pi*(sensor_data.index.day_of_year)/days_in_year).values\n",
    "\n",
    "days_in_week = 7\n",
    "sensor_data['sin_weekly'] = np.sin(2*np.pi*(sensor_data.index.day_of_week)/days_in_week).values\n",
    "sensor_data['cos_weekly'] = np.cos(2*np.pi*(sensor_data.index.day_of_week)/days_in_week).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = sensor_data.iloc[:int(sensor_data.shape[0]*0.9)]\n",
    "test_df = sensor_data.iloc[int(sensor_data.shape[0]*0.9):]\n",
    "\n",
    "train_val_split = int(train_val_df.shape[0]*0.8)\n",
    "train_df = train_val_df[:train_val_split]\n",
    "val_df = train_val_df[train_val_split:]\n",
    "\n",
    "train_labels = train_df.loc[:, sensor_data.columns.str.endswith('temperature')].drop(\"1OutdoorEntrance_temperature\", axis=1).sort_index(axis=1)\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "x_scaler.fit(train_df)\n",
    "y_scaler.fit(train_labels)\n",
    "\n",
    "# train_df_norm = scaler.fit_transform(train_df)\n",
    "# val_df_norm = scaler.transform(val_df)\n",
    "# test_df_norm = scaler.transform(test_df)\n",
    "\n",
    "\n",
    "print(f\"Shape of train_df = {train_df.shape}, Shape of val_df = {val_df.shape}, Shape of test_df = {test_df.shape}\")\n",
    "print(f\"Shape of train_labels = {train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import WindowGenerator\n",
    "import importlib\n",
    "importlib.reload(WindowGenerator)\n",
    "\n",
    "windows = WindowGenerator.WindowGenerator(4*24, 1, 0, train_df, val_df, test_df, train_labels.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.array(windows.train_data, dtype=object)[:,0]\n",
    "# y_train = np.array(windows.train_data, dtype=object)[:,1]\n",
    "\n",
    "# x_val = np.array(windows.val_data, dtype=object)[:,0]\n",
    "# y_val = np.array(windows.val_data, dtype=object)[:,1]\n",
    "\n",
    "# x_test = np.array(windows.test_data, dtype=object)[:,0]\n",
    "# y_test = np.array(windows.test_data, dtype=object)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm = []\n",
    "y_train_norm = []\n",
    "for inputs, labels in windows.train_data:\n",
    "    x_train_norm.append(x_scaler.transform(inputs))\n",
    "    y_train_norm.append(y_scaler.transform(labels))\n",
    "\n",
    "x_train_norm = np.array(x_train_norm)\n",
    "x_train_norm = torch.from_numpy(x_train_norm).float()\n",
    "y_train_norm = np.array(y_train_norm)\n",
    "y_train_norm = torch.from_numpy(y_train_norm.reshape(y_train_norm.shape[0], y_train_norm.shape[2])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_norm = []\n",
    "y_val_norm = []\n",
    "for inputs, labels in windows.val_data:\n",
    "    y_test_norm.append(x_scaler.transform(inputs))\n",
    "    y_val_norm.append(y_scaler.transform(labels))\n",
    "\n",
    "y_test_norm = np.array(y_test_norm)\n",
    "y_test_norm = torch.from_numpy(y_test_norm).float()\n",
    "y_val_norm = np.array(y_val_norm)\n",
    "y_val_norm = torch.from_numpy(y_val_norm.reshape(y_val_norm.shape[0], y_val_norm.shape[2])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_norm = []\n",
    "y_test_norm = []\n",
    "for inputs, labels in windows.test_data:\n",
    "    x_test_norm.append(x_scaler.transform(inputs))\n",
    "    y_test_norm.append(y_scaler.transform(labels))\n",
    "\n",
    "x_test_norm = np.array(x_test_norm)\n",
    "x_test_norm = torch.from_numpy(x_test_norm).float()\n",
    "y_test_norm = np.array(y_test_norm)\n",
    "y_test_norm = torch.from_numpy(y_test_norm.reshape(y_test_norm.shape[0], y_test_norm.shape[2])).float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_lstm_layers, linear_layers=[]):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_lstm_layers, batch_first=True)\n",
    "        if linear_layers == []:\n",
    "            self.linear_layers = [nn.Linear(hidden_size, output_size)]\n",
    "        else:\n",
    "            #He initialization\n",
    "            first_layer = nn.Linear(hidden_size, linear_layers[0])\n",
    "            first_layer.weight.data = torch.randn((hidden_size, linear_layers[0]))*np.sqrt(1/hidden_size)\n",
    "            first_layer.bias.data = torch.zeros(linear_layers[0])\n",
    "            self.linear_layers = [first_layer]\n",
    "\n",
    "            for i in range(len(linear_layers)-1):\n",
    "                n_in = linear_layers[i]\n",
    "                n_out = linear_layers[i+1]\n",
    "                layer = torch.nn.Linear(n_in, n_out)\n",
    "\n",
    "                layer.weight.data = torch.randn((n_out, n_in))*np.sqrt(2/n_in)\n",
    "                layer.bias.data = torch.zeros(n_out)\n",
    "                self.linear_layers.append(layer)\n",
    "            \n",
    "            last_layer = nn.Linear(linear_layers[-1], output_size)\n",
    "            last_layer.weight.data = torch.randn((n_out, n_in))*np.sqrt(2/n_in)\n",
    "            last_layer.bias.data = torch.zeros(n_out)\n",
    "            self.linear_layers.append(last_layer)\n",
    "\n",
    "        self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        self.act =nn.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        h_0 = Variable(torch.zeros(self.num_lstm_layers, input_seq.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_lstm_layers, input_seq.size(0), self.hidden_size)) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        # output, (hn, cn) = self.lstm(input_seq, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        # x = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        output, (hn, cn) = self.lstm(input_seq, (h_0, c_0))\n",
    "        x = hn.view(-1, self.hidden_size)\n",
    "        for layer in self.linear_layers:\n",
    "            x = self.act(x)\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        net: torch.nn.Module,\n",
    "        train_data: list,\n",
    "        val_data: list,\n",
    "        n_epochs: int,\n",
    "        lr: float,\n",
    "        l2_reg: float,\n",
    ") -> torch.nn.Module:\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # for inputs, targets in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        # net.hidden_cell = (torch.zeros(1, 1, net.hidden_size),\n",
    "        #                         torch.zeros(1, 1, net.hidden_size))\n",
    "\n",
    "        outputs = net(train_data[0])\n",
    "        batch_mse = criterion(outputs, train_data[1])\n",
    "        reg_loss = 0\n",
    "        for param in net.parameters():\n",
    "                reg_loss += param.abs().sum()\n",
    "        cost = batch_mse + l2_reg * reg_loss\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {cost.item()}')\n",
    "\n",
    "        # mse_val = 0\n",
    "        # if epoch % 10 == 0:\n",
    "        #         for inputs, label in val_data:\n",
    "        #                 pred = y_scaler.inverse_transform(net(inputs).detach().numpy())\n",
    "        #                 row_len = len(label[0])\n",
    "        #                 mse_val += np.sum(np.power(label-pred, 2))\n",
    "        #         mse_val /= len(val_data)*row_len\n",
    "        #         print(f'Epoch: {epoch+1}: Val MSE: {mse_val}')\n",
    "    return net\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size=35, hidden_size=128, output_size=17, num_lstm_layers=1)\n",
    "\n",
    "epochs = 500\n",
    "lr = 0.001\n",
    "l2_reg = 0\n",
    "train(net, [x_train_norm, y_train_norm], windows.test_data, epochs, lr, l2_reg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMINE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation data\n",
    "pred_val_inv = y_scaler.inverse_transform(net(y_test_norm).detach())\n",
    "# pred_val = torch.from_numpy(pred_val).to(torch.float)\n",
    "y_val_inv = y_scaler.inverse_transform(y_val_norm)\n",
    "\n",
    "# Compute MSE, MAE and MAPE on validation data\n",
    "print('Error on validation data')\n",
    "\n",
    "mse_val = np.mean(np.power((pred_val_inv - y_val_inv), 2))\n",
    "print(f'MSE: {mse_val.item()}')\n",
    "\n",
    "mae_val = np.mean(np.abs(pred_val_inv - y_val_inv))\n",
    "print(f'MAE: {mae_val.item()}')\n",
    "\n",
    "# mape_val = 100*np.mean(np.abs(np.divide(pred_val_inv - y_val_inv, y_val_inv)))\n",
    "# print(f'MAPE: {mape_val.item()} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "pred_train = y_scaler.inverse_transform(net(y_test_norm).detach().numpy())\n",
    "true_train = y_scaler.inverse_transform(y_val_norm.detach().numpy())\n",
    "net.train()\n",
    "\n",
    "fig,ax = plt.subplots(7,3,figsize=(15,25))\n",
    "for i in range(train_labels.shape[1]):\n",
    "    ax[i//3, i%3].plot(true_train[:, i], label=\"true\")\n",
    "    ax[i//3, i%3].plot(pred_train[:, i], '--', label=\"pred\")\n",
    "    ax[i//3, i%3].set_title(f\"{train_labels.columns[i]}\")\n",
    "    ax[i//3, i%3].legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Forecasting library"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3159e3b5aa562919176a089bbcda2fa6dcd8cffba80ba1d0a8857839d15440d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
